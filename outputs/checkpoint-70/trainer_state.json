{
  "best_metric": 1.092605471611023,
  "best_model_checkpoint": "outputs/checkpoint-70",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 70,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01,
      "grad_norm": 5.120645523071289,
      "learning_rate": 9.952380952380953e-05,
      "loss": 3.0122,
      "step": 1
    },
    {
      "epoch": 0.03,
      "grad_norm": 2.4026033878326416,
      "learning_rate": 9.904761904761905e-05,
      "loss": 2.4669,
      "step": 2
    },
    {
      "epoch": 0.04,
      "grad_norm": 2.365938663482666,
      "learning_rate": 9.857142857142858e-05,
      "loss": 2.4572,
      "step": 3
    },
    {
      "epoch": 0.06,
      "grad_norm": 2.0022354125976562,
      "learning_rate": 9.80952380952381e-05,
      "loss": 2.1368,
      "step": 4
    },
    {
      "epoch": 0.07,
      "grad_norm": 1.9722033739089966,
      "learning_rate": 9.761904761904762e-05,
      "loss": 1.9248,
      "step": 5
    },
    {
      "epoch": 0.09,
      "grad_norm": 1.6285748481750488,
      "learning_rate": 9.714285714285715e-05,
      "loss": 1.6472,
      "step": 6
    },
    {
      "epoch": 0.1,
      "grad_norm": 1.456742525100708,
      "learning_rate": 9.666666666666667e-05,
      "loss": 1.5213,
      "step": 7
    },
    {
      "epoch": 0.11,
      "grad_norm": 1.2314800024032593,
      "learning_rate": 9.61904761904762e-05,
      "loss": 1.5166,
      "step": 8
    },
    {
      "epoch": 0.13,
      "grad_norm": 1.0283551216125488,
      "learning_rate": 9.571428571428573e-05,
      "loss": 1.3025,
      "step": 9
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.8682361245155334,
      "learning_rate": 9.523809523809524e-05,
      "loss": 1.3363,
      "step": 10
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.9765923023223877,
      "learning_rate": 9.476190476190476e-05,
      "loss": 1.4606,
      "step": 11
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.96860271692276,
      "learning_rate": 9.428571428571429e-05,
      "loss": 1.3359,
      "step": 12
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.8567748069763184,
      "learning_rate": 9.380952380952381e-05,
      "loss": 1.3225,
      "step": 13
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.8928311467170715,
      "learning_rate": 9.333333333333334e-05,
      "loss": 1.2446,
      "step": 14
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.9519328474998474,
      "learning_rate": 9.285714285714286e-05,
      "loss": 1.2801,
      "step": 15
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.9169092774391174,
      "learning_rate": 9.238095238095239e-05,
      "loss": 1.1976,
      "step": 16
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.9239647388458252,
      "learning_rate": 9.19047619047619e-05,
      "loss": 1.3169,
      "step": 17
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.9516721367835999,
      "learning_rate": 9.142857142857143e-05,
      "loss": 1.3946,
      "step": 18
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.8438962697982788,
      "learning_rate": 9.095238095238096e-05,
      "loss": 1.0875,
      "step": 19
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.818696916103363,
      "learning_rate": 9.047619047619048e-05,
      "loss": 1.2671,
      "step": 20
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.7721073031425476,
      "learning_rate": 9e-05,
      "loss": 1.3657,
      "step": 21
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.8245837688446045,
      "learning_rate": 8.952380952380953e-05,
      "loss": 1.2839,
      "step": 22
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.829933226108551,
      "learning_rate": 8.904761904761905e-05,
      "loss": 1.2012,
      "step": 23
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.7955179810523987,
      "learning_rate": 8.857142857142857e-05,
      "loss": 1.3065,
      "step": 24
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.6953622102737427,
      "learning_rate": 8.80952380952381e-05,
      "loss": 1.1732,
      "step": 25
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.7300974726676941,
      "learning_rate": 8.761904761904762e-05,
      "loss": 1.1722,
      "step": 26
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.7761225700378418,
      "learning_rate": 8.714285714285715e-05,
      "loss": 1.3073,
      "step": 27
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.8698718547821045,
      "learning_rate": 8.666666666666667e-05,
      "loss": 1.2848,
      "step": 28
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.8517299294471741,
      "learning_rate": 8.61904761904762e-05,
      "loss": 1.3659,
      "step": 29
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.8180528879165649,
      "learning_rate": 8.571428571428571e-05,
      "loss": 1.1977,
      "step": 30
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.7853831648826599,
      "learning_rate": 8.523809523809524e-05,
      "loss": 1.233,
      "step": 31
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.795403003692627,
      "learning_rate": 8.476190476190477e-05,
      "loss": 1.2865,
      "step": 32
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.7057116627693176,
      "learning_rate": 8.428571428571429e-05,
      "loss": 1.1186,
      "step": 33
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.9095613360404968,
      "learning_rate": 8.380952380952382e-05,
      "loss": 1.2488,
      "step": 34
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.7100194692611694,
      "learning_rate": 8.333333333333334e-05,
      "loss": 1.1702,
      "step": 35
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.8228259086608887,
      "learning_rate": 8.285714285714287e-05,
      "loss": 1.3011,
      "step": 36
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.8088642954826355,
      "learning_rate": 8.238095238095238e-05,
      "loss": 1.2075,
      "step": 37
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.7242705821990967,
      "learning_rate": 8.19047619047619e-05,
      "loss": 1.3107,
      "step": 38
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.7341094613075256,
      "learning_rate": 8.142857142857143e-05,
      "loss": 1.2329,
      "step": 39
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.8294452428817749,
      "learning_rate": 8.095238095238096e-05,
      "loss": 1.3778,
      "step": 40
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.7858331799507141,
      "learning_rate": 8.047619047619048e-05,
      "loss": 1.248,
      "step": 41
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.703276515007019,
      "learning_rate": 8e-05,
      "loss": 1.2599,
      "step": 42
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.7194145917892456,
      "learning_rate": 7.952380952380952e-05,
      "loss": 1.2144,
      "step": 43
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.729668378829956,
      "learning_rate": 7.904761904761905e-05,
      "loss": 1.2199,
      "step": 44
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.672248363494873,
      "learning_rate": 7.857142857142858e-05,
      "loss": 1.1294,
      "step": 45
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.763018786907196,
      "learning_rate": 7.80952380952381e-05,
      "loss": 1.1468,
      "step": 46
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.8250757455825806,
      "learning_rate": 7.761904761904762e-05,
      "loss": 1.1402,
      "step": 47
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.7235530614852905,
      "learning_rate": 7.714285714285715e-05,
      "loss": 1.1692,
      "step": 48
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.6878651976585388,
      "learning_rate": 7.666666666666667e-05,
      "loss": 1.184,
      "step": 49
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.7022473812103271,
      "learning_rate": 7.619047619047618e-05,
      "loss": 1.218,
      "step": 50
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.7029778957366943,
      "learning_rate": 7.571428571428571e-05,
      "loss": 1.0951,
      "step": 51
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.7579712271690369,
      "learning_rate": 7.523809523809524e-05,
      "loss": 1.0942,
      "step": 52
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.9205188155174255,
      "learning_rate": 7.476190476190477e-05,
      "loss": 1.3795,
      "step": 53
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.7875801920890808,
      "learning_rate": 7.428571428571429e-05,
      "loss": 1.2646,
      "step": 54
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.7797236442565918,
      "learning_rate": 7.380952380952382e-05,
      "loss": 1.1664,
      "step": 55
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.7814587354660034,
      "learning_rate": 7.333333333333333e-05,
      "loss": 1.2552,
      "step": 56
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.7284800410270691,
      "learning_rate": 7.285714285714286e-05,
      "loss": 1.0282,
      "step": 57
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.6526520252227783,
      "learning_rate": 7.238095238095238e-05,
      "loss": 1.03,
      "step": 58
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.714015781879425,
      "learning_rate": 7.19047619047619e-05,
      "loss": 1.2104,
      "step": 59
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.7638450264930725,
      "learning_rate": 7.142857142857143e-05,
      "loss": 1.2127,
      "step": 60
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.6989523768424988,
      "learning_rate": 7.095238095238096e-05,
      "loss": 1.099,
      "step": 61
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.7760294675827026,
      "learning_rate": 7.047619047619048e-05,
      "loss": 1.2965,
      "step": 62
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.7859417796134949,
      "learning_rate": 7e-05,
      "loss": 1.1637,
      "step": 63
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.7201381921768188,
      "learning_rate": 6.952380952380952e-05,
      "loss": 1.1279,
      "step": 64
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.6948289275169373,
      "learning_rate": 6.904761904761905e-05,
      "loss": 1.1642,
      "step": 65
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.7719758152961731,
      "learning_rate": 6.857142857142858e-05,
      "loss": 1.2364,
      "step": 66
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.9616222381591797,
      "learning_rate": 6.80952380952381e-05,
      "loss": 1.2517,
      "step": 67
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.7812451124191284,
      "learning_rate": 6.761904761904763e-05,
      "loss": 1.2184,
      "step": 68
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.7739126682281494,
      "learning_rate": 6.714285714285714e-05,
      "loss": 1.1168,
      "step": 69
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.8631308674812317,
      "learning_rate": 6.666666666666667e-05,
      "loss": 1.2839,
      "step": 70
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.092605471611023,
      "eval_runtime": 167.6478,
      "eval_samples_per_second": 6.675,
      "eval_steps_per_second": 1.67,
      "step": 70
    }
  ],
  "logging_steps": 1,
  "max_steps": 210,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "total_flos": 5534560790077440.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
