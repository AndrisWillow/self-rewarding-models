{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0,
      "grad_norm": 1.4370098114013672,
      "learning_rate": 9.975000000000001e-05,
      "loss": 1.9538,
      "step": 1
    },
    {
      "epoch": 0.01,
      "grad_norm": 1.5545352697372437,
      "learning_rate": 9.95e-05,
      "loss": 1.649,
      "step": 2
    },
    {
      "epoch": 0.01,
      "grad_norm": 1.0926578044891357,
      "learning_rate": 9.925000000000001e-05,
      "loss": 1.4222,
      "step": 3
    },
    {
      "epoch": 0.01,
      "grad_norm": 1.2013851404190063,
      "learning_rate": 9.900000000000001e-05,
      "loss": 1.4625,
      "step": 4
    },
    {
      "epoch": 0.01,
      "grad_norm": 1.1583307981491089,
      "learning_rate": 9.875000000000002e-05,
      "loss": 1.1658,
      "step": 5
    },
    {
      "epoch": 0.01,
      "grad_norm": 1.0658516883850098,
      "learning_rate": 9.850000000000001e-05,
      "loss": 1.6474,
      "step": 6
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.9629778861999512,
      "learning_rate": 9.825e-05,
      "loss": 1.1305,
      "step": 7
    },
    {
      "epoch": 0.02,
      "grad_norm": 0.8224334716796875,
      "learning_rate": 9.8e-05,
      "loss": 1.4929,
      "step": 8
    },
    {
      "epoch": 0.02,
      "grad_norm": 1.16813325881958,
      "learning_rate": 9.775e-05,
      "loss": 1.2069,
      "step": 9
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.775199294090271,
      "learning_rate": 9.75e-05,
      "loss": 1.2184,
      "step": 10
    },
    {
      "epoch": 0.03,
      "grad_norm": 1.134354591369629,
      "learning_rate": 9.725e-05,
      "loss": 1.3465,
      "step": 11
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.6825536489486694,
      "learning_rate": 9.7e-05,
      "loss": 1.1074,
      "step": 12
    },
    {
      "epoch": 0.03,
      "grad_norm": 0.7160252332687378,
      "learning_rate": 9.675000000000001e-05,
      "loss": 1.2649,
      "step": 13
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.9219486117362976,
      "learning_rate": 9.65e-05,
      "loss": 1.1636,
      "step": 14
    },
    {
      "epoch": 0.04,
      "grad_norm": 1.0552923679351807,
      "learning_rate": 9.625000000000001e-05,
      "loss": 1.4319,
      "step": 15
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.544197142124176,
      "learning_rate": 9.6e-05,
      "loss": 1.1789,
      "step": 16
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.9693576097488403,
      "learning_rate": 9.575000000000001e-05,
      "loss": 1.3451,
      "step": 17
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.7621753215789795,
      "learning_rate": 9.55e-05,
      "loss": 1.2239,
      "step": 18
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.6239824295043945,
      "learning_rate": 9.525000000000001e-05,
      "loss": 1.4652,
      "step": 19
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.7100525498390198,
      "learning_rate": 9.5e-05,
      "loss": 1.0689,
      "step": 20
    },
    {
      "epoch": 0.05,
      "grad_norm": 0.8387598395347595,
      "learning_rate": 9.475e-05,
      "loss": 1.156,
      "step": 21
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.8167224526405334,
      "learning_rate": 9.449999999999999e-05,
      "loss": 1.4331,
      "step": 22
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.6715657114982605,
      "learning_rate": 9.425e-05,
      "loss": 1.1853,
      "step": 23
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.6269847750663757,
      "learning_rate": 9.4e-05,
      "loss": 1.0548,
      "step": 24
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.6803028583526611,
      "learning_rate": 9.375e-05,
      "loss": 1.4398,
      "step": 25
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.7096752524375916,
      "learning_rate": 9.350000000000001e-05,
      "loss": 1.5556,
      "step": 26
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.9452450275421143,
      "learning_rate": 9.325e-05,
      "loss": 1.2114,
      "step": 27
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.6132698059082031,
      "learning_rate": 9.300000000000001e-05,
      "loss": 0.7972,
      "step": 28
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.7628262639045715,
      "learning_rate": 9.275e-05,
      "loss": 1.0454,
      "step": 29
    },
    {
      "epoch": 0.07,
      "grad_norm": 0.7221527695655823,
      "learning_rate": 9.250000000000001e-05,
      "loss": 1.1908,
      "step": 30
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.6246254444122314,
      "learning_rate": 9.225e-05,
      "loss": 1.1849,
      "step": 31
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.5574566721916199,
      "learning_rate": 9.200000000000001e-05,
      "loss": 1.1184,
      "step": 32
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.58348548412323,
      "learning_rate": 9.175000000000001e-05,
      "loss": 0.9047,
      "step": 33
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.6186032295227051,
      "learning_rate": 9.15e-05,
      "loss": 1.7402,
      "step": 34
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.6737200021743774,
      "learning_rate": 9.125e-05,
      "loss": 1.2414,
      "step": 35
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.5548587441444397,
      "learning_rate": 9.1e-05,
      "loss": 1.2867,
      "step": 36
    },
    {
      "epoch": 0.09,
      "grad_norm": 0.626288652420044,
      "learning_rate": 9.075e-05,
      "loss": 1.4561,
      "step": 37
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.5942585468292236,
      "learning_rate": 9.05e-05,
      "loss": 0.9332,
      "step": 38
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.7095325589179993,
      "learning_rate": 9.025e-05,
      "loss": 0.7888,
      "step": 39
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.7281657457351685,
      "learning_rate": 9e-05,
      "loss": 1.4345,
      "step": 40
    },
    {
      "epoch": 0.1,
      "grad_norm": 1.054552674293518,
      "learning_rate": 8.975e-05,
      "loss": 1.3174,
      "step": 41
    },
    {
      "epoch": 0.1,
      "grad_norm": 0.787355363368988,
      "learning_rate": 8.950000000000001e-05,
      "loss": 1.2451,
      "step": 42
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.5347545146942139,
      "learning_rate": 8.925e-05,
      "loss": 1.0571,
      "step": 43
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.6502054333686829,
      "learning_rate": 8.900000000000001e-05,
      "loss": 1.016,
      "step": 44
    },
    {
      "epoch": 0.11,
      "grad_norm": 0.6905826330184937,
      "learning_rate": 8.875e-05,
      "loss": 1.2856,
      "step": 45
    },
    {
      "epoch": 0.12,
      "grad_norm": 1.0738937854766846,
      "learning_rate": 8.850000000000001e-05,
      "loss": 1.3817,
      "step": 46
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.9806605577468872,
      "learning_rate": 8.825e-05,
      "loss": 1.1526,
      "step": 47
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.5567650198936462,
      "learning_rate": 8.800000000000001e-05,
      "loss": 0.9419,
      "step": 48
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.8256539702415466,
      "learning_rate": 8.775e-05,
      "loss": 1.4434,
      "step": 49
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.8938298225402832,
      "learning_rate": 8.75e-05,
      "loss": 1.3264,
      "step": 50
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.5611056685447693,
      "learning_rate": 8.725e-05,
      "loss": 1.0782,
      "step": 51
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.6029524207115173,
      "learning_rate": 8.7e-05,
      "loss": 0.9335,
      "step": 52
    },
    {
      "epoch": 0.13,
      "grad_norm": 0.8078829646110535,
      "learning_rate": 8.675000000000001e-05,
      "loss": 1.1859,
      "step": 53
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.6101945638656616,
      "learning_rate": 8.65e-05,
      "loss": 1.194,
      "step": 54
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.698034942150116,
      "learning_rate": 8.625000000000001e-05,
      "loss": 1.3121,
      "step": 55
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.5965954661369324,
      "learning_rate": 8.6e-05,
      "loss": 0.8015,
      "step": 56
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.5143218636512756,
      "learning_rate": 8.575000000000001e-05,
      "loss": 0.9609,
      "step": 57
    },
    {
      "epoch": 0.14,
      "grad_norm": 0.7037180066108704,
      "learning_rate": 8.55e-05,
      "loss": 1.4025,
      "step": 58
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.5258134007453918,
      "learning_rate": 8.525000000000001e-05,
      "loss": 0.8433,
      "step": 59
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.6247049570083618,
      "learning_rate": 8.5e-05,
      "loss": 1.4215,
      "step": 60
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.6547890305519104,
      "learning_rate": 8.475000000000001e-05,
      "loss": 1.1019,
      "step": 61
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.6585288047790527,
      "learning_rate": 8.450000000000001e-05,
      "loss": 1.3954,
      "step": 62
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.6665820479393005,
      "learning_rate": 8.425e-05,
      "loss": 1.1105,
      "step": 63
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.5471152067184448,
      "learning_rate": 8.4e-05,
      "loss": 1.0016,
      "step": 64
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.5685873031616211,
      "learning_rate": 8.375e-05,
      "loss": 1.2666,
      "step": 65
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.6009711027145386,
      "learning_rate": 8.35e-05,
      "loss": 1.0271,
      "step": 66
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.5453159213066101,
      "learning_rate": 8.325e-05,
      "loss": 0.8752,
      "step": 67
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.6283100247383118,
      "learning_rate": 8.3e-05,
      "loss": 1.2762,
      "step": 68
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.5306984186172485,
      "learning_rate": 8.275e-05,
      "loss": 1.0995,
      "step": 69
    },
    {
      "epoch": 0.17,
      "grad_norm": 0.6446657776832581,
      "learning_rate": 8.25e-05,
      "loss": 1.1729,
      "step": 70
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.696731686592102,
      "learning_rate": 8.225000000000001e-05,
      "loss": 1.3082,
      "step": 71
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.6186133027076721,
      "learning_rate": 8.2e-05,
      "loss": 1.025,
      "step": 72
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.9627918601036072,
      "learning_rate": 8.175000000000001e-05,
      "loss": 1.428,
      "step": 73
    },
    {
      "epoch": 0.18,
      "grad_norm": 0.8437926769256592,
      "learning_rate": 8.15e-05,
      "loss": 1.6922,
      "step": 74
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.5913095474243164,
      "learning_rate": 8.125000000000001e-05,
      "loss": 1.1712,
      "step": 75
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.4879198670387268,
      "learning_rate": 8.1e-05,
      "loss": 0.9907,
      "step": 76
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.7426595687866211,
      "learning_rate": 8.075e-05,
      "loss": 1.2153,
      "step": 77
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.6501278877258301,
      "learning_rate": 8.05e-05,
      "loss": 1.2513,
      "step": 78
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.5104473233222961,
      "learning_rate": 8.025e-05,
      "loss": 0.6992,
      "step": 79
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.7100895643234253,
      "learning_rate": 8e-05,
      "loss": 1.1471,
      "step": 80
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.6552578210830688,
      "learning_rate": 7.975e-05,
      "loss": 1.2518,
      "step": 81
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.593109667301178,
      "learning_rate": 7.950000000000001e-05,
      "loss": 1.0477,
      "step": 82
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.8050844669342041,
      "learning_rate": 7.925e-05,
      "loss": 1.1386,
      "step": 83
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.6868867874145508,
      "learning_rate": 7.900000000000001e-05,
      "loss": 1.1397,
      "step": 84
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.5734004378318787,
      "learning_rate": 7.875e-05,
      "loss": 1.0469,
      "step": 85
    },
    {
      "epoch": 0.21,
      "grad_norm": 0.5970060229301453,
      "learning_rate": 7.850000000000001e-05,
      "loss": 1.5588,
      "step": 86
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.6120116710662842,
      "learning_rate": 7.825e-05,
      "loss": 0.8928,
      "step": 87
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.6206618547439575,
      "learning_rate": 7.800000000000001e-05,
      "loss": 0.9064,
      "step": 88
    },
    {
      "epoch": 0.22,
      "grad_norm": 1.1065454483032227,
      "learning_rate": 7.775e-05,
      "loss": 1.3534,
      "step": 89
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.8347197771072388,
      "learning_rate": 7.75e-05,
      "loss": 1.5609,
      "step": 90
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.7292149066925049,
      "learning_rate": 7.725e-05,
      "loss": 1.4051,
      "step": 91
    },
    {
      "epoch": 0.23,
      "grad_norm": 1.2227051258087158,
      "learning_rate": 7.7e-05,
      "loss": 1.545,
      "step": 92
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.7069385647773743,
      "learning_rate": 7.675e-05,
      "loss": 1.0112,
      "step": 93
    },
    {
      "epoch": 0.23,
      "grad_norm": 0.6725563406944275,
      "learning_rate": 7.65e-05,
      "loss": 0.9955,
      "step": 94
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.551135778427124,
      "learning_rate": 7.625e-05,
      "loss": 1.1231,
      "step": 95
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.5932391881942749,
      "learning_rate": 7.6e-05,
      "loss": 1.0214,
      "step": 96
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.5588952302932739,
      "learning_rate": 7.575e-05,
      "loss": 1.1707,
      "step": 97
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.6444432735443115,
      "learning_rate": 7.55e-05,
      "loss": 1.365,
      "step": 98
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.7562464475631714,
      "learning_rate": 7.525e-05,
      "loss": 1.2058,
      "step": 99
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.7407156229019165,
      "learning_rate": 7.500000000000001e-05,
      "loss": 1.1678,
      "step": 100
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.7203980684280396,
      "learning_rate": 7.475000000000001e-05,
      "loss": 0.9871,
      "step": 101
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.7119991183280945,
      "learning_rate": 7.450000000000001e-05,
      "loss": 1.1724,
      "step": 102
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.5981650352478027,
      "learning_rate": 7.425e-05,
      "loss": 0.9874,
      "step": 103
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.636749267578125,
      "learning_rate": 7.4e-05,
      "loss": 1.0399,
      "step": 104
    },
    {
      "epoch": 0.26,
      "grad_norm": 1.019991159439087,
      "learning_rate": 7.375e-05,
      "loss": 1.5368,
      "step": 105
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.7112401723861694,
      "learning_rate": 7.35e-05,
      "loss": 1.0806,
      "step": 106
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.7564159035682678,
      "learning_rate": 7.325e-05,
      "loss": 1.6715,
      "step": 107
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.751045286655426,
      "learning_rate": 7.3e-05,
      "loss": 1.3205,
      "step": 108
    },
    {
      "epoch": 0.27,
      "grad_norm": 0.8242149949073792,
      "learning_rate": 7.275e-05,
      "loss": 1.6112,
      "step": 109
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.9429209232330322,
      "learning_rate": 7.25e-05,
      "loss": 1.1745,
      "step": 110
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.7245813012123108,
      "learning_rate": 7.225000000000001e-05,
      "loss": 1.4396,
      "step": 111
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.7103298902511597,
      "learning_rate": 7.2e-05,
      "loss": 1.0123,
      "step": 112
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.5631832480430603,
      "learning_rate": 7.175000000000001e-05,
      "loss": 0.8405,
      "step": 113
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.5845197439193726,
      "learning_rate": 7.15e-05,
      "loss": 1.1096,
      "step": 114
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.7122868895530701,
      "learning_rate": 7.125000000000001e-05,
      "loss": 1.1982,
      "step": 115
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.8113924860954285,
      "learning_rate": 7.1e-05,
      "loss": 1.3066,
      "step": 116
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.7288076281547546,
      "learning_rate": 7.075e-05,
      "loss": 1.4074,
      "step": 117
    },
    {
      "epoch": 0.29,
      "grad_norm": 0.6195992827415466,
      "learning_rate": 7.05e-05,
      "loss": 0.997,
      "step": 118
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.49886491894721985,
      "learning_rate": 7.025e-05,
      "loss": 1.0156,
      "step": 119
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.7443699240684509,
      "learning_rate": 7e-05,
      "loss": 1.164,
      "step": 120
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.5876154899597168,
      "learning_rate": 6.975e-05,
      "loss": 1.0174,
      "step": 121
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.7880350947380066,
      "learning_rate": 6.95e-05,
      "loss": 1.0972,
      "step": 122
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.914929211139679,
      "learning_rate": 6.925e-05,
      "loss": 1.6887,
      "step": 123
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.5718399286270142,
      "learning_rate": 6.9e-05,
      "loss": 1.4182,
      "step": 124
    },
    {
      "epoch": 0.31,
      "grad_norm": 0.6822056174278259,
      "learning_rate": 6.875e-05,
      "loss": 1.0198,
      "step": 125
    },
    {
      "epoch": 0.32,
      "grad_norm": 1.2164666652679443,
      "learning_rate": 6.850000000000001e-05,
      "loss": 1.3639,
      "step": 126
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.6589890718460083,
      "learning_rate": 6.825e-05,
      "loss": 1.3504,
      "step": 127
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.6611905694007874,
      "learning_rate": 6.800000000000001e-05,
      "loss": 1.2592,
      "step": 128
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.631659746170044,
      "learning_rate": 6.775000000000001e-05,
      "loss": 0.9381,
      "step": 129
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.8243575096130371,
      "learning_rate": 6.750000000000001e-05,
      "loss": 1.0719,
      "step": 130
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.6456419229507446,
      "learning_rate": 6.725000000000001e-05,
      "loss": 1.211,
      "step": 131
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.46278730034828186,
      "learning_rate": 6.7e-05,
      "loss": 0.8883,
      "step": 132
    },
    {
      "epoch": 0.33,
      "grad_norm": 0.6296877264976501,
      "learning_rate": 6.675e-05,
      "loss": 1.155,
      "step": 133
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.5689494609832764,
      "learning_rate": 6.65e-05,
      "loss": 1.1647,
      "step": 134
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.6793057322502136,
      "learning_rate": 6.625e-05,
      "loss": 1.8093,
      "step": 135
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.6655106544494629,
      "learning_rate": 6.6e-05,
      "loss": 1.0891,
      "step": 136
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.633757472038269,
      "learning_rate": 6.575e-05,
      "loss": 1.2215,
      "step": 137
    },
    {
      "epoch": 0.34,
      "grad_norm": 0.6546434164047241,
      "learning_rate": 6.55e-05,
      "loss": 0.9154,
      "step": 138
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.7553698420524597,
      "learning_rate": 6.525e-05,
      "loss": 1.008,
      "step": 139
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.8240171670913696,
      "learning_rate": 6.500000000000001e-05,
      "loss": 1.1266,
      "step": 140
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.8762897253036499,
      "learning_rate": 6.475e-05,
      "loss": 1.277,
      "step": 141
    },
    {
      "epoch": 0.35,
      "grad_norm": 0.6563883423805237,
      "learning_rate": 6.450000000000001e-05,
      "loss": 0.9812,
      "step": 142
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.5798113942146301,
      "learning_rate": 6.425e-05,
      "loss": 1.1612,
      "step": 143
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.6410357356071472,
      "learning_rate": 6.400000000000001e-05,
      "loss": 1.3936,
      "step": 144
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.5846341252326965,
      "learning_rate": 6.375e-05,
      "loss": 1.21,
      "step": 145
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.5113514065742493,
      "learning_rate": 6.35e-05,
      "loss": 0.9859,
      "step": 146
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.621910035610199,
      "learning_rate": 6.324999999999999e-05,
      "loss": 1.0683,
      "step": 147
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.5778119564056396,
      "learning_rate": 6.3e-05,
      "loss": 1.0622,
      "step": 148
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.524178683757782,
      "learning_rate": 6.275e-05,
      "loss": 1.1789,
      "step": 149
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.8122841715812683,
      "learning_rate": 6.25e-05,
      "loss": 1.3758,
      "step": 150
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.8089959621429443,
      "learning_rate": 6.225000000000001e-05,
      "loss": 1.1876,
      "step": 151
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.6515034437179565,
      "learning_rate": 6.2e-05,
      "loss": 1.1143,
      "step": 152
    },
    {
      "epoch": 0.38,
      "grad_norm": 0.9187893867492676,
      "learning_rate": 6.175000000000001e-05,
      "loss": 1.2156,
      "step": 153
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.5716566443443298,
      "learning_rate": 6.15e-05,
      "loss": 1.0688,
      "step": 154
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.6477764248847961,
      "learning_rate": 6.125000000000001e-05,
      "loss": 1.3611,
      "step": 155
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.8080136179924011,
      "learning_rate": 6.1e-05,
      "loss": 1.3427,
      "step": 156
    },
    {
      "epoch": 0.39,
      "grad_norm": 0.5139898061752319,
      "learning_rate": 6.0750000000000006e-05,
      "loss": 1.0122,
      "step": 157
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.6349061131477356,
      "learning_rate": 6.05e-05,
      "loss": 1.2568,
      "step": 158
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.6721336841583252,
      "learning_rate": 6.025000000000001e-05,
      "loss": 0.851,
      "step": 159
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.6445418000221252,
      "learning_rate": 6e-05,
      "loss": 1.4682,
      "step": 160
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.7479758262634277,
      "learning_rate": 5.975000000000001e-05,
      "loss": 1.5253,
      "step": 161
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.8236134648323059,
      "learning_rate": 5.95e-05,
      "loss": 1.4624,
      "step": 162
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.8637672662734985,
      "learning_rate": 5.9250000000000004e-05,
      "loss": 1.4511,
      "step": 163
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.6245541572570801,
      "learning_rate": 5.9e-05,
      "loss": 0.956,
      "step": 164
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.6316689252853394,
      "learning_rate": 5.8750000000000005e-05,
      "loss": 1.1972,
      "step": 165
    },
    {
      "epoch": 0.41,
      "grad_norm": 0.814392626285553,
      "learning_rate": 5.85e-05,
      "loss": 1.2504,
      "step": 166
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.6323995590209961,
      "learning_rate": 5.8250000000000006e-05,
      "loss": 1.0794,
      "step": 167
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.5179756283760071,
      "learning_rate": 5.8e-05,
      "loss": 1.2068,
      "step": 168
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.6667309999465942,
      "learning_rate": 5.775e-05,
      "loss": 1.2678,
      "step": 169
    },
    {
      "epoch": 0.42,
      "grad_norm": 0.5681968331336975,
      "learning_rate": 5.7499999999999995e-05,
      "loss": 0.9067,
      "step": 170
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.7443954348564148,
      "learning_rate": 5.725e-05,
      "loss": 1.1127,
      "step": 171
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.556693434715271,
      "learning_rate": 5.6999999999999996e-05,
      "loss": 0.7879,
      "step": 172
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.6693456172943115,
      "learning_rate": 5.6750000000000004e-05,
      "loss": 0.8963,
      "step": 173
    },
    {
      "epoch": 0.43,
      "grad_norm": 0.59306800365448,
      "learning_rate": 5.65e-05,
      "loss": 1.1507,
      "step": 174
    },
    {
      "epoch": 0.44,
      "grad_norm": 1.0352435111999512,
      "learning_rate": 5.6250000000000005e-05,
      "loss": 1.504,
      "step": 175
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.5427584648132324,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 1.1686,
      "step": 176
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.5394023060798645,
      "learning_rate": 5.575e-05,
      "loss": 0.8243,
      "step": 177
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.5513260960578918,
      "learning_rate": 5.550000000000001e-05,
      "loss": 1.1627,
      "step": 178
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.7252210378646851,
      "learning_rate": 5.525e-05,
      "loss": 1.1108,
      "step": 179
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.8640585541725159,
      "learning_rate": 5.500000000000001e-05,
      "loss": 1.238,
      "step": 180
    },
    {
      "epoch": 0.45,
      "grad_norm": 0.6089819073677063,
      "learning_rate": 5.475e-05,
      "loss": 0.8956,
      "step": 181
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.6894915699958801,
      "learning_rate": 5.45e-05,
      "loss": 1.0569,
      "step": 182
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.5874022245407104,
      "learning_rate": 5.4250000000000004e-05,
      "loss": 1.1327,
      "step": 183
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.6927613019943237,
      "learning_rate": 5.4000000000000005e-05,
      "loss": 1.0938,
      "step": 184
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.7674168348312378,
      "learning_rate": 5.375e-05,
      "loss": 1.1189,
      "step": 185
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.5485216975212097,
      "learning_rate": 5.3500000000000006e-05,
      "loss": 0.9792,
      "step": 186
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.8477780222892761,
      "learning_rate": 5.325e-05,
      "loss": 0.9485,
      "step": 187
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.6299400329589844,
      "learning_rate": 5.300000000000001e-05,
      "loss": 1.1023,
      "step": 188
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.555232048034668,
      "learning_rate": 5.275e-05,
      "loss": 0.7798,
      "step": 189
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.5289170145988464,
      "learning_rate": 5.25e-05,
      "loss": 1.1578,
      "step": 190
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.6748149991035461,
      "learning_rate": 5.2249999999999996e-05,
      "loss": 1.4301,
      "step": 191
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.6899154186248779,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 1.36,
      "step": 192
    },
    {
      "epoch": 0.48,
      "grad_norm": 1.2093021869659424,
      "learning_rate": 5.175e-05,
      "loss": 1.5566,
      "step": 193
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.7507549524307251,
      "learning_rate": 5.1500000000000005e-05,
      "loss": 1.2781,
      "step": 194
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.5343111157417297,
      "learning_rate": 5.125e-05,
      "loss": 0.9465,
      "step": 195
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.9561426043510437,
      "learning_rate": 5.1000000000000006e-05,
      "loss": 1.4042,
      "step": 196
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.599838376045227,
      "learning_rate": 5.075e-05,
      "loss": 1.2772,
      "step": 197
    },
    {
      "epoch": 0.49,
      "grad_norm": 0.8561608195304871,
      "learning_rate": 5.05e-05,
      "loss": 1.2498,
      "step": 198
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.7260428667068481,
      "learning_rate": 5.0249999999999995e-05,
      "loss": 1.0717,
      "step": 199
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.7312738299369812,
      "learning_rate": 5e-05,
      "loss": 1.4146,
      "step": 200
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.5466701984405518,
      "learning_rate": 4.975e-05,
      "loss": 1.1376,
      "step": 201
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.5955657958984375,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 1.1002,
      "step": 202
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.6497791409492493,
      "learning_rate": 4.9250000000000004e-05,
      "loss": 1.1151,
      "step": 203
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.7377645969390869,
      "learning_rate": 4.9e-05,
      "loss": 1.0881,
      "step": 204
    },
    {
      "epoch": 0.51,
      "grad_norm": 0.774880051612854,
      "learning_rate": 4.875e-05,
      "loss": 1.4085,
      "step": 205
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.6050125956535339,
      "learning_rate": 4.85e-05,
      "loss": 1.1078,
      "step": 206
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.6130536794662476,
      "learning_rate": 4.825e-05,
      "loss": 0.8065,
      "step": 207
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.0492087602615356,
      "learning_rate": 4.8e-05,
      "loss": 1.5275,
      "step": 208
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.5820105671882629,
      "learning_rate": 4.775e-05,
      "loss": 1.2191,
      "step": 209
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.6789676547050476,
      "learning_rate": 4.75e-05,
      "loss": 1.027,
      "step": 210
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.7237703800201416,
      "learning_rate": 4.7249999999999997e-05,
      "loss": 1.038,
      "step": 211
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.645682692527771,
      "learning_rate": 4.7e-05,
      "loss": 1.2382,
      "step": 212
    },
    {
      "epoch": 0.53,
      "grad_norm": 0.7801156640052795,
      "learning_rate": 4.6750000000000005e-05,
      "loss": 0.9466,
      "step": 213
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.7731950283050537,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 0.9496,
      "step": 214
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.749967098236084,
      "learning_rate": 4.6250000000000006e-05,
      "loss": 1.3223,
      "step": 215
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.5839129686355591,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.1338,
      "step": 216
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.5171973705291748,
      "learning_rate": 4.575e-05,
      "loss": 0.9365,
      "step": 217
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.6584581136703491,
      "learning_rate": 4.55e-05,
      "loss": 1.1308,
      "step": 218
    },
    {
      "epoch": 0.55,
      "grad_norm": 1.038372278213501,
      "learning_rate": 4.525e-05,
      "loss": 1.0816,
      "step": 219
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.6492726802825928,
      "learning_rate": 4.5e-05,
      "loss": 1.0951,
      "step": 220
    },
    {
      "epoch": 0.55,
      "grad_norm": 0.8117026090621948,
      "learning_rate": 4.4750000000000004e-05,
      "loss": 1.1037,
      "step": 221
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.7114768624305725,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 1.089,
      "step": 222
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.6254478096961975,
      "learning_rate": 4.4250000000000005e-05,
      "loss": 1.1507,
      "step": 223
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.5640682578086853,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.2459,
      "step": 224
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.6083704829216003,
      "learning_rate": 4.375e-05,
      "loss": 0.9415,
      "step": 225
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.8981022238731384,
      "learning_rate": 4.35e-05,
      "loss": 1.0815,
      "step": 226
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.6388649344444275,
      "learning_rate": 4.325e-05,
      "loss": 1.0226,
      "step": 227
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.6227911710739136,
      "learning_rate": 4.3e-05,
      "loss": 1.0649,
      "step": 228
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.4540335536003113,
      "learning_rate": 4.275e-05,
      "loss": 0.9148,
      "step": 229
    },
    {
      "epoch": 0.57,
      "grad_norm": 0.7567789554595947,
      "learning_rate": 4.25e-05,
      "loss": 1.288,
      "step": 230
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.806027889251709,
      "learning_rate": 4.2250000000000004e-05,
      "loss": 1.2282,
      "step": 231
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.5664778351783752,
      "learning_rate": 4.2e-05,
      "loss": 1.0348,
      "step": 232
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.6360569596290588,
      "learning_rate": 4.175e-05,
      "loss": 1.0167,
      "step": 233
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.7255047559738159,
      "learning_rate": 4.15e-05,
      "loss": 1.1464,
      "step": 234
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.651770293712616,
      "learning_rate": 4.125e-05,
      "loss": 1.2144,
      "step": 235
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.5627903342247009,
      "learning_rate": 4.1e-05,
      "loss": 0.8701,
      "step": 236
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.602262556552887,
      "learning_rate": 4.075e-05,
      "loss": 1.4178,
      "step": 237
    },
    {
      "epoch": 0.59,
      "grad_norm": 0.572361946105957,
      "learning_rate": 4.05e-05,
      "loss": 0.9548,
      "step": 238
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.5307790040969849,
      "learning_rate": 4.025e-05,
      "loss": 0.9644,
      "step": 239
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.6698300242424011,
      "learning_rate": 4e-05,
      "loss": 1.0823,
      "step": 240
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.608359694480896,
      "learning_rate": 3.9750000000000004e-05,
      "loss": 1.3625,
      "step": 241
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.5589064359664917,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 0.977,
      "step": 242
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.8116940259933472,
      "learning_rate": 3.9250000000000005e-05,
      "loss": 1.4703,
      "step": 243
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.6493832468986511,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.0695,
      "step": 244
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.6639237403869629,
      "learning_rate": 3.875e-05,
      "loss": 1.2111,
      "step": 245
    },
    {
      "epoch": 0.61,
      "grad_norm": 0.6350428462028503,
      "learning_rate": 3.85e-05,
      "loss": 0.9926,
      "step": 246
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.8493349552154541,
      "learning_rate": 3.825e-05,
      "loss": 1.3126,
      "step": 247
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.701398491859436,
      "learning_rate": 3.8e-05,
      "loss": 1.1325,
      "step": 248
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.8072353601455688,
      "learning_rate": 3.775e-05,
      "loss": 1.2708,
      "step": 249
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.6577618718147278,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 1.2452,
      "step": 250
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.685268223285675,
      "learning_rate": 3.7250000000000004e-05,
      "loss": 1.0872,
      "step": 251
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.7040485739707947,
      "learning_rate": 3.7e-05,
      "loss": 1.3937,
      "step": 252
    },
    {
      "epoch": 0.63,
      "grad_norm": 0.5456728935241699,
      "learning_rate": 3.675e-05,
      "loss": 1.2325,
      "step": 253
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.762182354927063,
      "learning_rate": 3.65e-05,
      "loss": 1.4007,
      "step": 254
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.7946609854698181,
      "learning_rate": 3.625e-05,
      "loss": 1.0215,
      "step": 255
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.5679858922958374,
      "learning_rate": 3.6e-05,
      "loss": 0.9359,
      "step": 256
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.6066341996192932,
      "learning_rate": 3.575e-05,
      "loss": 0.7707,
      "step": 257
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.7447749972343445,
      "learning_rate": 3.55e-05,
      "loss": 1.0257,
      "step": 258
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.7627590298652649,
      "learning_rate": 3.525e-05,
      "loss": 0.9462,
      "step": 259
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.6160465478897095,
      "learning_rate": 3.5e-05,
      "loss": 1.2663,
      "step": 260
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.6265660524368286,
      "learning_rate": 3.475e-05,
      "loss": 1.0665,
      "step": 261
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.5174753069877625,
      "learning_rate": 3.45e-05,
      "loss": 0.8915,
      "step": 262
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.5984307527542114,
      "learning_rate": 3.4250000000000006e-05,
      "loss": 0.9978,
      "step": 263
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.5371080636978149,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.9918,
      "step": 264
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.7364964485168457,
      "learning_rate": 3.375000000000001e-05,
      "loss": 1.0909,
      "step": 265
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.5392734408378601,
      "learning_rate": 3.35e-05,
      "loss": 1.235,
      "step": 266
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.9188899993896484,
      "learning_rate": 3.325e-05,
      "loss": 1.0274,
      "step": 267
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.9791931509971619,
      "learning_rate": 3.3e-05,
      "loss": 1.3373,
      "step": 268
    },
    {
      "epoch": 0.67,
      "grad_norm": 0.8799360394477844,
      "learning_rate": 3.275e-05,
      "loss": 1.282,
      "step": 269
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.596348226070404,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 1.3462,
      "step": 270
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.8159042596817017,
      "learning_rate": 3.2250000000000005e-05,
      "loss": 1.1422,
      "step": 271
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.6016926765441895,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.1193,
      "step": 272
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.6177650094032288,
      "learning_rate": 3.175e-05,
      "loss": 1.0187,
      "step": 273
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.5426310300827026,
      "learning_rate": 3.15e-05,
      "loss": 0.8618,
      "step": 274
    },
    {
      "epoch": 0.69,
      "grad_norm": 1.0095608234405518,
      "learning_rate": 3.125e-05,
      "loss": 1.7928,
      "step": 275
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.8153873682022095,
      "learning_rate": 3.1e-05,
      "loss": 1.4596,
      "step": 276
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.6159945130348206,
      "learning_rate": 3.075e-05,
      "loss": 0.9304,
      "step": 277
    },
    {
      "epoch": 0.69,
      "grad_norm": 0.8365696668624878,
      "learning_rate": 3.05e-05,
      "loss": 1.3118,
      "step": 278
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.73466557264328,
      "learning_rate": 3.025e-05,
      "loss": 1.3685,
      "step": 279
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.5447717308998108,
      "learning_rate": 3e-05,
      "loss": 1.1613,
      "step": 280
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.5327112078666687,
      "learning_rate": 2.975e-05,
      "loss": 1.2583,
      "step": 281
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.7818405032157898,
      "learning_rate": 2.95e-05,
      "loss": 1.4432,
      "step": 282
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.8299893140792847,
      "learning_rate": 2.925e-05,
      "loss": 1.3958,
      "step": 283
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.5215173363685608,
      "learning_rate": 2.9e-05,
      "loss": 0.9882,
      "step": 284
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.7708006501197815,
      "learning_rate": 2.8749999999999997e-05,
      "loss": 1.3433,
      "step": 285
    },
    {
      "epoch": 0.71,
      "grad_norm": 0.7262600660324097,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 1.0059,
      "step": 286
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.4895762801170349,
      "learning_rate": 2.825e-05,
      "loss": 1.005,
      "step": 287
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.6103769540786743,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.9214,
      "step": 288
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.5428786873817444,
      "learning_rate": 2.7750000000000004e-05,
      "loss": 1.0137,
      "step": 289
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.6825891733169556,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 1.1924,
      "step": 290
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.7074849605560303,
      "learning_rate": 2.725e-05,
      "loss": 1.0968,
      "step": 291
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.698055624961853,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 1.5221,
      "step": 292
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.4841020107269287,
      "learning_rate": 2.6750000000000003e-05,
      "loss": 0.904,
      "step": 293
    },
    {
      "epoch": 0.73,
      "grad_norm": 0.991272509098053,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 1.5444,
      "step": 294
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.4877219498157501,
      "learning_rate": 2.625e-05,
      "loss": 0.8527,
      "step": 295
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.5232833623886108,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.9609,
      "step": 296
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.5412139296531677,
      "learning_rate": 2.5750000000000002e-05,
      "loss": 0.9227,
      "step": 297
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.692293107509613,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 0.9645,
      "step": 298
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.7822495698928833,
      "learning_rate": 2.525e-05,
      "loss": 1.381,
      "step": 299
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.5404425263404846,
      "learning_rate": 2.5e-05,
      "loss": 1.0722,
      "step": 300
    },
    {
      "epoch": 0.75,
      "grad_norm": 0.6939840316772461,
      "learning_rate": 2.4750000000000002e-05,
      "loss": 1.3068,
      "step": 301
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.6607188582420349,
      "learning_rate": 2.45e-05,
      "loss": 1.083,
      "step": 302
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.5467957854270935,
      "learning_rate": 2.425e-05,
      "loss": 1.2893,
      "step": 303
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.6939576864242554,
      "learning_rate": 2.4e-05,
      "loss": 1.0835,
      "step": 304
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.5474487543106079,
      "learning_rate": 2.375e-05,
      "loss": 0.993,
      "step": 305
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.6416115760803223,
      "learning_rate": 2.35e-05,
      "loss": 0.8494,
      "step": 306
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.5328856706619263,
      "learning_rate": 2.3250000000000003e-05,
      "loss": 1.1194,
      "step": 307
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.8482739925384521,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 1.2175,
      "step": 308
    },
    {
      "epoch": 0.77,
      "grad_norm": 0.7243398427963257,
      "learning_rate": 2.275e-05,
      "loss": 1.5671,
      "step": 309
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.6899664998054504,
      "learning_rate": 2.25e-05,
      "loss": 1.1565,
      "step": 310
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.6828698515892029,
      "learning_rate": 2.2250000000000002e-05,
      "loss": 1.286,
      "step": 311
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.6561704277992249,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 1.1852,
      "step": 312
    },
    {
      "epoch": 0.78,
      "grad_norm": 0.6790251731872559,
      "learning_rate": 2.175e-05,
      "loss": 1.0111,
      "step": 313
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.7723875045776367,
      "learning_rate": 2.15e-05,
      "loss": 1.2806,
      "step": 314
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.6676434874534607,
      "learning_rate": 2.125e-05,
      "loss": 1.0057,
      "step": 315
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.968964159488678,
      "learning_rate": 2.1e-05,
      "loss": 1.2989,
      "step": 316
    },
    {
      "epoch": 0.79,
      "grad_norm": 0.8715186715126038,
      "learning_rate": 2.075e-05,
      "loss": 1.4856,
      "step": 317
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.6900588870048523,
      "learning_rate": 2.05e-05,
      "loss": 1.1666,
      "step": 318
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.8363454937934875,
      "learning_rate": 2.025e-05,
      "loss": 1.0532,
      "step": 319
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.8405276536941528,
      "learning_rate": 2e-05,
      "loss": 1.557,
      "step": 320
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.6025381684303284,
      "learning_rate": 1.9750000000000002e-05,
      "loss": 0.9516,
      "step": 321
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.9135564565658569,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 1.6621,
      "step": 322
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.6995735764503479,
      "learning_rate": 1.925e-05,
      "loss": 1.0974,
      "step": 323
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.6085559129714966,
      "learning_rate": 1.9e-05,
      "loss": 1.0642,
      "step": 324
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.5601396560668945,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 1.0191,
      "step": 325
    },
    {
      "epoch": 0.81,
      "grad_norm": 0.584996223449707,
      "learning_rate": 1.85e-05,
      "loss": 1.3456,
      "step": 326
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.5521953105926514,
      "learning_rate": 1.825e-05,
      "loss": 0.8862,
      "step": 327
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.648227870464325,
      "learning_rate": 1.8e-05,
      "loss": 0.9625,
      "step": 328
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.530514657497406,
      "learning_rate": 1.775e-05,
      "loss": 1.0124,
      "step": 329
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.6190344095230103,
      "learning_rate": 1.75e-05,
      "loss": 1.0199,
      "step": 330
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.5139830708503723,
      "learning_rate": 1.725e-05,
      "loss": 1.036,
      "step": 331
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.525600790977478,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.7755,
      "step": 332
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.7713873982429504,
      "learning_rate": 1.675e-05,
      "loss": 1.2093,
      "step": 333
    },
    {
      "epoch": 0.83,
      "grad_norm": 0.5498810410499573,
      "learning_rate": 1.65e-05,
      "loss": 1.1038,
      "step": 334
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.6209690570831299,
      "learning_rate": 1.6250000000000002e-05,
      "loss": 1.1885,
      "step": 335
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.5395841598510742,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.9956,
      "step": 336
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.7808248996734619,
      "learning_rate": 1.575e-05,
      "loss": 1.1052,
      "step": 337
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.5219237804412842,
      "learning_rate": 1.55e-05,
      "loss": 0.8926,
      "step": 338
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.684020459651947,
      "learning_rate": 1.525e-05,
      "loss": 1.287,
      "step": 339
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.674336850643158,
      "learning_rate": 1.5e-05,
      "loss": 1.2407,
      "step": 340
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.5995393395423889,
      "learning_rate": 1.475e-05,
      "loss": 0.9341,
      "step": 341
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.6887087821960449,
      "learning_rate": 1.45e-05,
      "loss": 0.9057,
      "step": 342
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.6905421018600464,
      "learning_rate": 1.4249999999999999e-05,
      "loss": 1.5316,
      "step": 343
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.5603955388069153,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 1.149,
      "step": 344
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.7870843410491943,
      "learning_rate": 1.3750000000000002e-05,
      "loss": 0.9682,
      "step": 345
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.5328119397163391,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 0.825,
      "step": 346
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.8528227806091309,
      "learning_rate": 1.3250000000000002e-05,
      "loss": 1.1633,
      "step": 347
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.542949378490448,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.8947,
      "step": 348
    },
    {
      "epoch": 0.87,
      "grad_norm": 0.8440186977386475,
      "learning_rate": 1.2750000000000002e-05,
      "loss": 1.2198,
      "step": 349
    },
    {
      "epoch": 0.88,
      "grad_norm": 1.3351408243179321,
      "learning_rate": 1.25e-05,
      "loss": 1.1172,
      "step": 350
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.48654308915138245,
      "learning_rate": 1.225e-05,
      "loss": 0.9369,
      "step": 351
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.7712548971176147,
      "learning_rate": 1.2e-05,
      "loss": 1.4092,
      "step": 352
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.5238673090934753,
      "learning_rate": 1.175e-05,
      "loss": 0.9461,
      "step": 353
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.6620408892631531,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 1.0187,
      "step": 354
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.9833635091781616,
      "learning_rate": 1.125e-05,
      "loss": 1.1858,
      "step": 355
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.5822675824165344,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 1.06,
      "step": 356
    },
    {
      "epoch": 0.89,
      "grad_norm": 0.5547046065330505,
      "learning_rate": 1.075e-05,
      "loss": 1.0461,
      "step": 357
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.5382124185562134,
      "learning_rate": 1.05e-05,
      "loss": 0.9019,
      "step": 358
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.5526502132415771,
      "learning_rate": 1.025e-05,
      "loss": 1.4029,
      "step": 359
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.8345406651496887,
      "learning_rate": 1e-05,
      "loss": 1.2023,
      "step": 360
    },
    {
      "epoch": 0.9,
      "grad_norm": 0.5308232307434082,
      "learning_rate": 9.750000000000002e-06,
      "loss": 1.1497,
      "step": 361
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.5411974191665649,
      "learning_rate": 9.5e-06,
      "loss": 0.8772,
      "step": 362
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.6065505146980286,
      "learning_rate": 9.25e-06,
      "loss": 0.759,
      "step": 363
    },
    {
      "epoch": 0.91,
      "grad_norm": 1.3483076095581055,
      "learning_rate": 9e-06,
      "loss": 1.6019,
      "step": 364
    },
    {
      "epoch": 0.91,
      "grad_norm": 0.5625425577163696,
      "learning_rate": 8.75e-06,
      "loss": 0.9318,
      "step": 365
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.52353835105896,
      "learning_rate": 8.500000000000002e-06,
      "loss": 0.8615,
      "step": 366
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.7386488914489746,
      "learning_rate": 8.25e-06,
      "loss": 1.2972,
      "step": 367
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.8532356023788452,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.4805,
      "step": 368
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.565903902053833,
      "learning_rate": 7.75e-06,
      "loss": 1.3813,
      "step": 369
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.7190890312194824,
      "learning_rate": 7.5e-06,
      "loss": 1.3065,
      "step": 370
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.6292433142662048,
      "learning_rate": 7.25e-06,
      "loss": 1.414,
      "step": 371
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.6441233158111572,
      "learning_rate": 7.000000000000001e-06,
      "loss": 1.2214,
      "step": 372
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.6375762224197388,
      "learning_rate": 6.750000000000001e-06,
      "loss": 1.2985,
      "step": 373
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.5196938514709473,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 1.0326,
      "step": 374
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.863847553730011,
      "learning_rate": 6.25e-06,
      "loss": 1.2765,
      "step": 375
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.5030035376548767,
      "learning_rate": 6e-06,
      "loss": 0.9331,
      "step": 376
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.6097831130027771,
      "learning_rate": 5.750000000000001e-06,
      "loss": 0.9097,
      "step": 377
    },
    {
      "epoch": 0.94,
      "grad_norm": 0.5735069513320923,
      "learning_rate": 5.500000000000001e-06,
      "loss": 0.8883,
      "step": 378
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.6100573539733887,
      "learning_rate": 5.25e-06,
      "loss": 1.2575,
      "step": 379
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.7069256901741028,
      "learning_rate": 5e-06,
      "loss": 1.2896,
      "step": 380
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.5532976984977722,
      "learning_rate": 4.75e-06,
      "loss": 0.7526,
      "step": 381
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.5271669030189514,
      "learning_rate": 4.5e-06,
      "loss": 1.1352,
      "step": 382
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.6083055138587952,
      "learning_rate": 4.250000000000001e-06,
      "loss": 1.1425,
      "step": 383
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.5161686539649963,
      "learning_rate": 4.000000000000001e-06,
      "loss": 1.0712,
      "step": 384
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.6994760036468506,
      "learning_rate": 3.75e-06,
      "loss": 1.2707,
      "step": 385
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.8276574611663818,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 1.1889,
      "step": 386
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.5451138019561768,
      "learning_rate": 3.2500000000000002e-06,
      "loss": 1.0103,
      "step": 387
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.6390916705131531,
      "learning_rate": 3e-06,
      "loss": 1.1161,
      "step": 388
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.575541079044342,
      "learning_rate": 2.7500000000000004e-06,
      "loss": 1.0503,
      "step": 389
    },
    {
      "epoch": 0.97,
      "grad_norm": 0.818891704082489,
      "learning_rate": 2.5e-06,
      "loss": 1.0445,
      "step": 390
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.5969653129577637,
      "learning_rate": 2.25e-06,
      "loss": 1.0602,
      "step": 391
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.5058515071868896,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.783,
      "step": 392
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.6466135382652283,
      "learning_rate": 1.7500000000000002e-06,
      "loss": 0.9343,
      "step": 393
    },
    {
      "epoch": 0.98,
      "grad_norm": 0.6244078874588013,
      "learning_rate": 1.5e-06,
      "loss": 0.9953,
      "step": 394
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.582611083984375,
      "learning_rate": 1.25e-06,
      "loss": 1.0013,
      "step": 395
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.6395156383514404,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 1.0546,
      "step": 396
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.5966886281967163,
      "learning_rate": 7.5e-07,
      "loss": 1.1957,
      "step": 397
    },
    {
      "epoch": 0.99,
      "grad_norm": 0.8708509206771851,
      "learning_rate": 5.000000000000001e-07,
      "loss": 1.1585,
      "step": 398
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.5931495428085327,
      "learning_rate": 2.5000000000000004e-07,
      "loss": 1.0825,
      "step": 399
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.5669375061988831,
      "learning_rate": 0.0,
      "loss": 1.038,
      "step": 400
    }
  ],
  "logging_steps": 1,
  "max_steps": 400,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "total_flos": 7.01274072809472e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
